{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cebbfec1",
   "metadata": {},
   "source": [
    "# Feature Selection and Suggestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59e75ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn.compose import ColumnTransformer\n",
    "from typing import Optional, Tuple, Dict, List\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.feature_selection import SelectFromModel \n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler \n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, classification_report, confusion_matrix\n",
    "import os\n",
    "from typing import List, Dict, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57492630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (4700, 31)\n",
      "Data dictionary shape: (31, 11)\n"
     ]
    }
   ],
   "source": [
    "# variable class\n",
    "class analyze:\n",
    "    def __init__(self,\n",
    "                 data_source: str = os.path.join(os.getcwd(),\"Project Data.csv\"), # analysis data\n",
    "                 data_dictionary_source:str = os.path.join(os.getcwd(),\"Project Data Dictionary.xlsx\"), # data dictionary\n",
    "                 output_prefix:str = \"iteration\", # appends to indicate output\n",
    "                 target_col:str = \"success\", # modeling target (1=good on C1B, 0=bad on C1B)\n",
    "                 exclude_col:List[str] = ['ssn_ssn'], # columns to exclude from modeling\n",
    "                 top_n_features:int = 10, # how many top features to print at the end\n",
    "                 ):\n",
    "        self.data_source:str = data_source\n",
    "        self.data_dictionary_source:str = data_dictionary_source\n",
    "        self.output_prefix:str = output_prefix\n",
    "        self.target_col:str = target_col\n",
    "        self.exclude_col:List[str] = exclude_col\n",
    "        self.top_n_features:int = top_n_features\n",
    "        self.data_table()\n",
    "        print(f\"Data shape: {self.data.shape}\")\n",
    "        self.data_dictionary_table()\n",
    "        print(f\"Data dictionary shape: {self.data_dictionary.shape}\")\n",
    "        self.data = self.create_target(self.data)\n",
    "        # Additional cleaning of data columns? Unnecessary? Seems the dataset is currently pretty clean.\n",
    "        # Keep only rows that have a target for training - David\n",
    "            # we should only have success == 1 for the training of the model\n",
    "        # NOTE: STEP 2 — DICTIONARY-DRIVEN CLEANING (ONE DICTIONARY) \n",
    "            # May not need to happen here.\n",
    "        # NOTE: STEP 3 — MODEL: L1 feature selection → LightGBM (or RF) → Top N features \n",
    "            # start here!\n",
    "                # NOTE: we need to also do grid-search\n",
    "                    # Or, rather, we need to iterate through all the options in the parameters\n",
    "                        # to find the optimal condition\n",
    "    def data_table(self) -> None:\n",
    "        self.data:pd.DataFrame = (\n",
    "            pd.read_csv(self.data_source)\n",
    "        )\n",
    "    def data_dictionary_table(self) -> None:\n",
    "        self.data_dictionary:pd.DataFrame = (\n",
    "            pd.read_excel(self.data_dictionary_source)\n",
    "        )\n",
    "    def create_target(self,\n",
    "                      data:pd.DataFrame\n",
    "                      ) -> pd.DataFrame:\n",
    "        data[\"success\"] = (data['active_account']==True) & (data['delinquent_account']==False)\n",
    "        data:pd.DataFrame = data.drop(columns=['active_account','delinquent_account'])\n",
    "        return data\n",
    "    \n",
    "# Create the analysis class object\n",
    "analysis:analyze = analyze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5b51dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ident_monitor_opt         float64\n",
      "Num_Bk_Accts              float64\n",
      "income                    float64\n",
      "bk_accts_ssn              float64\n",
      "cells_ssn                 float64\n",
      "dls_ssn                   float64\n",
      "emails_ssn                float64\n",
      "hmphones_ssn              float64\n",
      "addrs_ssn                 float64\n",
      "ssn_ssn                   float64\n",
      "zips_ssn                  float64\n",
      "empl_ssn_6Mo              float64\n",
      "pday_inq_15days           float64\n",
      "dti_score                 float64\n",
      "days_from_registration    float64\n",
      "days_from_login           float64\n",
      "asset_score               float64\n",
      "alt_risk_score            float64\n",
      "seg_id                    float64\n",
      "stability_score           float64\n",
      "max_bkaccts               float64\n",
      "auto_inq_1dy              float64\n",
      "auto_inq_7dy              float64\n",
      "auto_inq_72hr             float64\n",
      "cc_inq_72hr               float64\n",
      "cc_inq_10dy               float64\n",
      "pl_inq_72hr               float64\n",
      "pl_inq_90dy               float64\n",
      "alt_risk_score_2          float64\n",
      "success                      bool\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "table:pd.DataFrame = analysis.data\n",
    "print(table.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900988d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y:pd.Series = table[analysis.target_col] # is bool fine?\n",
    "X:pd.DataFrame = table.drop(columns=analysis.exclude_col)\n",
    "\n",
    "# Separate numeric vs categorical (objects) \n",
    "cat_cols = [c for c in X.columns if X[c].dtype == object] \n",
    "num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "# OneHotEncoder: handle new/old sklearn\n",
    "try:\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True) \n",
    "except TypeError:\n",
    "    # older sklearn\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=True)\n",
    "\n",
    "num_pipe = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scale\", StandardScaler(with_mean=True, with_std=True)),\n",
    "])\n",
    "\n",
    "cat_pipe = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", ohe),\n",
    "])\n",
    "\n",
    "pre = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", num_pipe, num_cols),\n",
    "        (\"cat\", cat_pipe, cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size=0.30, \n",
    "    random_state=42, \n",
    "    stratify=y,\n",
    ")\n",
    "\n",
    "# --- Fit the preprocessor first, then do L1 selection on the preprocessed matrix --- \n",
    "pre.fit(X_train, y_train) \n",
    "Xtr_pre = pre.transform(X_train) \n",
    "Xte_pre = pre.transform(X_test)\n",
    "\n",
    "# L1 logistic for feature selection (wrapped inside SelectFromModel; no pipeline step for plain LogisticRegression) \n",
    "lasso = LogisticRegression(\n",
    "    penalty=\"l1\",\n",
    "    solver=\"liblinear\",\n",
    "    max_iter=3000,\n",
    "    class_weight=\"balanced\",\n",
    ")\n",
    "\n",
    "selector = SelectFromModel(\n",
    "    estimator=lasso, \n",
    "    threshold=\"median\",\n",
    "    ) \n",
    "selector.fit(\n",
    "    Xtr_pre, \n",
    "    y_train,\n",
    "    )\n",
    "\n",
    "Xtr_sel = selector.transform(Xtr_pre)\n",
    "Xte_sel = selector.transform(Xte_pre)\n",
    "try:\n",
    "    print(f\"Selected features (post-L1): {Xtr_sel.shape[1]}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to print due to {e}\")\n",
    "\n",
    "# Final model: LightGBM (fallback to RF if LGBM not installed)\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier\n",
    "    clf = LGBMClassifier(\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.03,\n",
    "        num_leaves=64,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_lambda=1.0,\n",
    "        class_weight=\"balanced\",\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    model_name = \"LightGBM\"\n",
    "except Exception:\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=500,\n",
    "        min_samples_leaf=2,\n",
    "        class_weight=\"balanced\",\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    model_name = \"RandomForest\"\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
